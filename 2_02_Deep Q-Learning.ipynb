{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "# Forzar uso CPU en caso de dispones GPU en el PC\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.- Deep Q-Learning\n",
    "\n",
    "\n",
    "* El Deep Q-Learning es una técnica de aprendizaje por refuerzo similar a la técnica del Q-Learning, con la diferencia de que la Q-Table \"*la aprende*\" una red neuronal en vez de ir calculandola con la Q-Function.\n",
    "\n",
    "\n",
    "* Para ello se necesita definir una red neuronal que dado un estado como entrada, nos devuelva los valores **Q(s,)** de las diferentes acciones que se puedan tomar, seleccionando como mejor opción aquella que tenga el mayor valor de Q.\n",
    "\n",
    "\n",
    "* Por tanto la red neuronal tendrá que tener la siguiente estructura:\n",
    "\n",
    "    - ***Capa de entrada:*** tendrá las ***neurona necesarias para codificar el estado en el que se encuentra el agente dentro del entorno***\n",
    "    - Capa ocultas. El número de capas ocultas y el numero de neuronas por capa dependerá de la complejidad del problema.\n",
    "    - ***Capa de salida:*** tendrá tantas ***neuronas como acciones pueda realizar el agente en el entorno***, representando la salida de cada neurona el valor Q(s,a).\n",
    "    \n",
    "\n",
    "<img src=\"./imgs/017_RL.png\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "* En el Deep Q-Learning a diferencia del Q-Learning y el SARSA no se aprende por cada accion realizada por el agente, si no que en ***cada episodio se deja que el agente tome las acciones que considere, bien sea explorando o explotando el modelo*** (dado por la red neuronal). Cada una de las acciones que toma el agente debe de guardarse en una memoria (***Experience Replay***) que debe tener la siguiente informacion:\n",
    "\n",
    "    - Estado actual $s_t$\n",
    "    - Accion realizada $a$\n",
    "    - Reward $R(s_t,a)$\n",
    "    - Estado siguiente $s_{t+1}$\n",
    "    - ¿Es el estado final?\n",
    "\n",
    "\n",
    "* Una vez que el agente termina el episodio es el momento de seleccionar un conjunto de acciones para que la red neuronal aprenda de ellas. La selección de estas acciones dependerá del problema a resolver, siendo una estrategia de selección la selección aleatoria de *K* acciones.\n",
    "\n",
    "\n",
    "* Cuando ya tenemos seleccionas las acciones del episodio, es el momento de entrenar la red neuronal. Para ello iremos actualizando los pesos de la red tras aprender de cada una de las acciones.\n",
    "\n",
    "\n",
    "* El proceso de aprendizaje de la red neuronal sería la siguiente:\n",
    "\n",
    "    1.- Dado el estado $s_t$, calculamos los $Q(s_t,)$ que nos da la red neuronal; es decir, le pedimos una predicción.\n",
    "     \n",
    "    2.- Obtenemos el mejor valor de $Q(s_t,)$; es decir, obtenemos:\n",
    "    $$Q_{predict}(s_t,a) = \\underset{{a}}{max} Q({s_t},{a})$$\n",
    "    \n",
    "    3.- Obtenemos el valor $Q(s_t,a)$ dado por:\n",
    "    $$Q_{objetivo}(s_t, a) = R(s_t,a) + \\gamma \\cdot \\underset{{a'}}{max} Q({s_{t+1}},{a'})$$\n",
    "    \n",
    "    4.- Entrenamos la red, con el objetivo de que aprenda el nuevo valor $Q(s_t,a)$; por tanto, la entrada de la red neuronal será $s_t$ y tendrá que ajustar los pesos para predecir el nuevo valor $Q(s_t,a)$ que es el calculado por $Q_{objetivo}(s_t,a)$.\n",
    "    \n",
    "    \n",
    "* Si nos fijamos en el proceso la red neuronal sabe en un momento dado un $Q(s_t,a)$ y tras realizar una acción tenemos un nuevo valor $\\widehat{Q}(s_t, a)$ que hemos llamado $Q_{objetivo}(s_t, a)$ y que se calcula con la Q-Function. Si decimos a la red que use como función de perdida el error cuadrático medio (MSE) lo que esta haciendo es tomar como error la diferencia temporal:\n",
    "\n",
    "\n",
    "$$Loss = \\frac{1}{2} \\sum{((R(s_t,a) + \\gamma \\cdot \\underset{a}{max} Q(s_{t+1},{a}')) - Q(s_t, a))^2} = \\frac{1}{2} \\sum{TD(a,s_t)^2}$$\n",
    "\n",
    "\n",
    "* De esta manera, la red neuronal ira aprendiendo poco a poco (en función de su learnin rate) los valores $Q(s,a)$:\n",
    "\n",
    "$$Q(s, a) :=  Q(s, a) + \\alpha \\cdot TD(a,s)$$\n",
    "\n",
    "* El Pseudocódigo de este algoritmo es el siguiente:\n",
    "\n",
    "\n",
    "<img src=\"./imgs/019_RL.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "## Ejemplo con Deep Q-Learning\n",
    "\n",
    "\n",
    "* A continuación vamos a resolver un problema con Aprendizaje por refuerzo usando el algoritmo del Deep Q-Learning.\n",
    "\n",
    "\n",
    "* El problema que queremos resolver es el de encontrar el camino que nos suponga una mayor recompensa (el más corto) desde un estado inicial $[0,0]$ hasta el estado final $[4,4]$, pudiendo realizar 4 tipos de acciones:\n",
    "\n",
    "\n",
    "<img src=\"./imgs/007_RL.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "* Dado este problema, debemos de definir una red neuronal con la siguiente arquitectura:\n",
    "\n",
    "    - Capa de entrada de 2 neuronas (una para la posición X y otro para la posición Y)\n",
    "    - Capas ocultas (a definir)\n",
    "    - Capa de salida de 4 neuronas (una por cada acción a tomar {arriba, abajo, izquierda, derecha} en el estado)\n",
    "\n",
    "<img src=\"./imgs/018_RL.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "* Para resolver este problema vamos a realizar lo siguiente:\n",
    "<span></span><br>\n",
    "    1. [Definición del entorno](#M1)\n",
    "<span></span><br>\n",
    "    2. [Deep Q-Learner: Implementación](#M2)\n",
    "<span></span><br>\n",
    "    3. [Ejecución: Entorno - Agente](#M3)\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "## <a name=\"M1\">1.- Definición del entorno</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las librerias necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Formato de los decimales en Pandas y la semilla del Random\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "np.random.seed(5)\n",
    "\n",
    "\n",
    "class Environment(object):\n",
    "    def __init__(self, action_penalty=-1.0):\n",
    "        \"\"\"\n",
    "        Clase que representa y controla en entorno\n",
    "        :param action_penalty:    Factor de descuento del Reward por acción tomada\n",
    "        \"\"\"\n",
    "        self.actions = {'Arriba': [-1, 0],\n",
    "                        'Abajo': [1, 0],\n",
    "                        'Izquierda': [0, -1],\n",
    "                        'Derecha': [0, 1]}\n",
    "        self.rewards = [[0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, -100.0, 100.0]]\n",
    "        self.action_penalty = action_penalty         # Penalización por cada paso dado\n",
    "        self.state = [0, 0]                          # Estado en el que se encuentra el agente\n",
    "        self.final_state = [3, 3]                    # Estado final del entorno. Cuando el agente llega, se termina el episodio\n",
    "        self.total_reward = 0.0                      # Contador de recompensas en el episodio\n",
    "        self.actions_done = []                       # Lista en la que se guardan los pasos (acciones) realizadas en cada episodio\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Método que reinicia las variables del entorno y devuelve es estado inicial\n",
    "        :return:    state\n",
    "        \"\"\"\n",
    "        self.total_reward = 0.0    # Inicializamos Reward a 0\n",
    "        self.state = [0, 0]        # Posicionamos al agente en el estado inicial\n",
    "        self.actions_done = []     # Inicializamos la listas de pasos (acciones)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Método que ejecuta una acción determinada del conjunto de acciones {Arriba, Abajo, Izquierda, Derecha}\n",
    "        para guiar al agente en el entorno.\n",
    "        :param action:    Acción a ejecutar\n",
    "        :return:          (state, reward, is_final_state)\n",
    "        \"\"\"\n",
    "        self.apply_action(action)                                                  # Realizamos la acción (cambio de estado)\n",
    "        self.actions_done.append(self.state[:])                                    # Guardamos el paso (accion) realizada\n",
    "        is_final_state = np.array_equal(self.state, self.final_state)              # Comprobamos si hemos llegado al estado final\n",
    "        reward = self.rewards[self.state[0]][self.state[1]] + self.action_penalty  # Calculamos el reward (recompensa) por la acción tomada\n",
    "        self.total_reward += reward                                                # Sumamos el reward (recompensa) total del episodio\n",
    "        return self.state, reward, is_final_state                                  # Devolvemos es estado, el reward (recompensa) y si hemos llegado al estado final\n",
    "\n",
    "    def apply_action(self, action):\n",
    "        \"\"\"\n",
    "        Método que calcula el nuevo estado a partir de la acción a ejecutar\n",
    "        :param action:    Acción a ejecutar\n",
    "        \"\"\"\n",
    "        self.state[0] += self.actions[action][0]\n",
    "        self.state[1] += self.actions[action][1]\n",
    "\n",
    "        # Si nos salimos del tablero por arriba o por abajo, nos quedamos en la posicion que estabamos\n",
    "        if self.state[0] < 0:\n",
    "            self.state[0] = 0\n",
    "        elif self.state[0] > len(self.rewards) - 1:\n",
    "            self.state[0] -= 1\n",
    "\n",
    "        # Si nos salimos del tablero por los lados, nos quedamos en la posicion que estabamos\n",
    "        if self.state[1] < 0:\n",
    "            self.state[1] = 0\n",
    "        elif self.state[1] > len(self.rewards[0]) - 1:\n",
    "            self.state[1] -= 1\n",
    "\n",
    "    def print_path_episode(self):\n",
    "        \"\"\"\n",
    "        Método que imprime por pantalla el camino seguido por el agente\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        path = [['-' for _ in range(len(self.rewards))] for _ in range(len(self.rewards[0]))]\n",
    "        path[0][0] = '0'\n",
    "        for index, step in enumerate(self.actions_done):\n",
    "            path[step[0]][step[1]] = str(index + 1)\n",
    "\n",
    "        print(pd.DataFrame(data=np.array([np.array(xi) for xi in path]),\n",
    "                           index=[\"x{}\".format(str(i)) for i in range(len(path))],\n",
    "                           columns=[\"y{}\".format(str(i)) for i in range(len(path[0]))]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## <a name=\"M2\">2.- Deep Q-Learner: Implementación</a>\n",
    "\n",
    "\n",
    "* Recordemos el Pseudocódigo del Algoritmo:\n",
    "\n",
    "\n",
    "<img src=\"./imgs/019_RL.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class DeepQLearner(object):\n",
    "\n",
    "    def __init__(self, environment, max_memory=100, discount_factor=0.1, explotation_rate=0.95, max_steps=500):\n",
    "        \"\"\"\n",
    "        Clase que implementa el Algoritmo de Aprendizaje Deep Q-Learning\n",
    "        :param environment:         Entorno en el que tomar las acciones\n",
    "        :param max_memory:          Número maximo de acciones a memorizar (guardar) en un episodio\n",
    "        :param discount_factor:     Factor de descuento (0=Estrategia a corto plazo, 1=Estrategia a largo plazo)\n",
    "        :param explotation_rate:    Ratio de explotación\n",
    "        :param max_steps:           Número máximo de pasos a ejecutar en un episodio\n",
    "        \"\"\"\n",
    "        self.environment = environment\n",
    "        self.memory = list()               # Estado Actual (S_t), Acción realizada (a_t), Reward (R(s,t)), Estado Siguiente (S_t+1), ¿Estado Final?\n",
    "        self.max_memory = max_memory\n",
    "        self.model = self.create_model()   # Red Neuronal\n",
    "        self.discount_factor = discount_factor\n",
    "        self.max_explotation_rate = explotation_rate\n",
    "        self.explotation_rate = 0\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'Deep Q-Learner'\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Función que crea y devuelve la red neuronal\n",
    "        :return: Red Neuronal\n",
    "        \"\"\"\n",
    "\n",
    "        input_dim = len(self.environment.state)      # Número de neuronas de la capa de entrada '2' (X,Y)\n",
    "        output_dim = len(self.environment.actions)   # Número de neuronas de la capa de salida '4' (estados)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, activation='relu', input_dim=input_dim))\n",
    "        model.add(Dense(output_dim, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_next_action(self, state):\n",
    "        \"\"\"\n",
    "        Método que selecciona la siguiente acción a tomar:\n",
    "            Aleatoria ->     si el ratio de explotación es inferior al umbral\n",
    "            Mejor Acción ->  si el ratio de explotación es superior al umbral\n",
    "        :param state:   Estado del agente\n",
    "        :return:        next_action\n",
    "        \"\"\"\n",
    "\n",
    "        if np.random.uniform() > self.explotation_rate:\n",
    "            # Seleccionamos una acción al azar\n",
    "            next_action = np.random.choice(list(self.environment.actions))\n",
    "        else:\n",
    "            # Seleccionamos la acción que nos de mayor valor.\n",
    "            qus = self.model.predict([state])\n",
    "            idx_action = np.argmax(qus[0])\n",
    "            next_action = list(self.environment.actions)[idx_action]\n",
    "\n",
    "        return next_action\n",
    "\n",
    "    def update(self, environment, state, action, reward, new_state, is_final_state, num_episode, num_steps):\n",
    "        \"\"\"\n",
    "        Método que implementa el Algoritmo de Aprendizaje Deep Q-Learning\n",
    "        :param environment:       Entorno en el que tomar las acciones\n",
    "        :param state:             Estado actual\n",
    "        :param action:            Acción a realizar\n",
    "        :param reward:            Recompensa obtenida por la acción tomada\n",
    "        :param new_state:         Nuevo estado al que se mueve el agente\n",
    "        :param is_final_state:    Boolean. Devuelve True si el agente llega al estado final; si no, False\n",
    "        :param num_episode:       Número de episodios ejecutados\n",
    "        :param num_steps:         Número de pasos dados en el episodio\n",
    "        \"\"\"\n",
    "        self.remenber(state=state, action=action, reward=reward, new_state=new_state, is_final_state=is_final_state)\n",
    "        if is_final_state or num_steps > self.max_steps:\n",
    "            self.learn(environment=environment, num_episode=num_episode)\n",
    "            self.reset()\n",
    "\n",
    "    def remenber(self, state, action, reward, new_state, is_final_state):\n",
    "        \"\"\"\n",
    "        Método que guarda en una lista, una tupla con información de cada uno de los pasos\n",
    "        realizados por el agente en el entorno durante el episodio. En el caso de que el numero\n",
    "        de acciones en la memoria sea superior al número de acciones máximas a guardar, iremos\n",
    "        eliminando la acción más antigua de la lista.\n",
    "        :param state:             Estado actual\n",
    "        :param action:            Acción a realizar\n",
    "        :param reward:            Recompensa obtenida por la acción tomada\n",
    "        :param new_state:         Nuevo estado al que se mueve el agente\n",
    "        :param is_final_state:    Boolean. Devuelve True si el agente llega al estado final; si no, False\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory.append((state, action, reward, new_state, is_final_state))\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def learn(self, environment, num_episode):\n",
    "        \"\"\"\n",
    "        Método que actualiza el modelo (Red Neuronal) - Aprende de las acciones realizadas en el episodio.\n",
    "        Este método también actualiza el ratio de explotación de las siguiente manera:\n",
    "        ration_explotacion = ratio_explotación - (maximo_ratio_explotacion / (num_episodios + 1))\n",
    "        :param environment:       Entorno en el que tomar las acciones\n",
    "        :param num_episode:       Número del episodio\n",
    "        \"\"\"\n",
    "        batch = (random.sample(self.memory, 100)\n",
    "                 if len(self.memory) > 100 else random.sample(self.memory, len(self.memory)))\n",
    "\n",
    "        for state, action, reward, new_state, is_final_state in batch:\n",
    "            q_values = self.model.predict([state])\n",
    "            idx_action = list(environment.actions).index(action)\n",
    "\n",
    "            q_values[0][idx_action] = (reward + (self.discount_factor * np.amax(self.model.predict([new_state])[0]))\n",
    "                                       if not is_final_state else reward)\n",
    "\n",
    "            self.model.fit(np.array([state]), q_values, epochs=1, verbose=0)\n",
    "\n",
    "        # Actualizo el ratio de explotación\n",
    "        self.explotation_rate = self.max_explotation_rate - (self.max_explotation_rate / (num_episode + 1))\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Método que vacia la lista (la memoria) con los pasos realizados por el agente\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        del self.memory[:]\n",
    "\n",
    "    def print_q_table(self):\n",
    "        \"\"\"\n",
    "        Método que imprime por pantalla la Q-Table aprendida por la red\n",
    "        \"\"\"\n",
    "        states = list(itertools.product([0, 1, 2, 3], repeat=2))  # Generamos todos los posibles estados\n",
    "        q_table = self.model.predict(states)                      # Predecimos con la red los Q(s,a)\n",
    "        df = (pd.DataFrame(data=q_table,                          # Pasamos la Q_Table a un DataFrame\n",
    "                           columns=['Arriba', 'Abajo', 'Izquierda', 'Derecha']))\n",
    "        df.insert(0, 'Estado', ['x{},y{}'.format(state[0], state[1]) for state in states])\n",
    "        print(df.to_string(index=False))\n",
    "\n",
    "    def print_best_actions_states(self):\n",
    "        \"\"\"\n",
    "        Método que imprime por pantalla el valor de la mejor opción a realizar en cada uno de los estados\n",
    "        \"\"\"\n",
    "        states = list(itertools.product([0, 1, 2, 3], repeat=2))  # Generamos todos los posibles estados\n",
    "        q_table = self.model.predict(states)  # Predecimos con la red los Q(s,a)\n",
    "\n",
    "        best = (np.array([list(self.environment.actions)[np.argmax(row)] for row in q_table])\n",
    "                .reshape(len(self.environment.rewards), len(self.environment.rewards[0])))\n",
    "\n",
    "        print(pd.DataFrame(data=np.array([np.array(xi) for xi in best]),\n",
    "                           index=[\"x{}\".format(str(i)) for i in range(len(best))],\n",
    "                           columns=[\"y{}\".format(str(i)) for i in range(len(best[0]))]))\n",
    "\n",
    "    def print_best_values_states(self):\n",
    "        \"\"\"\n",
    "        Método que imprime por pantalla el valor de la mejor opción a realizar en cada uno de los estados\n",
    "        \"\"\"\n",
    "        states = list(itertools.product([0, 1, 2, 3], repeat=2))  # Generamos todos los posibles estados\n",
    "        q_table = self.model.predict(states)                      # Predecimos con la red los Q(s,a)\n",
    "\n",
    "        best = (np.array([[np.max(row) for row in q_table]])\n",
    "                .reshape(len(self.environment.rewards), len(self.environment.rewards[0])))\n",
    "\n",
    "        print(pd.DataFrame(data=np.array([np.array(xi) for xi in best]),\n",
    "                           index=[\"x{}\".format(str(i)) for i in range(len(best))],\n",
    "                           columns=[\"y{}\".format(str(i)) for i in range(len(best[0]))]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## <a name=\"M3\">3.- Ejecución: Entorno - Agente</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def run_agent(learner=DeepQLearner, num_episodes=10, learning_rate=0.1, discount_factor=0.1, ratio_explotacion=0.95,\n",
    "              max_steps=500, verbose=False):\n",
    "    \"\"\"\n",
    "    Método que ejecuta el proceso de aprendizaje del agente en un entorno\n",
    "    :param learner:              Algoritmo de Aprendizaje\n",
    "    :param num_episodes:         Número de veces que se ejecuta (o aprende) el agente en el entorno\n",
    "    :param learning_rate:        Factor de Aprendizaje\n",
    "    :param discount_factor:      Factor de descuento (0=Estrategia a corto plazo, 1=Estrategia a largo plazo)\n",
    "    :param ratio_explotacion:    Ratio de explotación\n",
    "    :param max_steps:            TODO\n",
    "    :param verbose:              Boolean, si queremos o no imprimir por pantalla información del proceso\n",
    "    \"\"\"\n",
    "\n",
    "    # Instanciamos el entorno\n",
    "    environment = Environment()\n",
    "\n",
    "    # Instanciamos el método de aprendizaje\n",
    "    learner = learner(environment=environment,\n",
    "                      max_memory=100,\n",
    "                      discount_factor=discount_factor,\n",
    "                      explotation_rate=ratio_explotacion,\n",
    "                      max_steps=max_steps)\n",
    "\n",
    "    last_episode = None\n",
    "\n",
    "    for n_episode in range(0, num_episodes):\n",
    "        state = environment.reset()\n",
    "        is_final_state = False\n",
    "        num_steps_episode = 0\n",
    "        while not is_final_state:\n",
    "            old_state = state[:]\n",
    "            next_action = learner.get_next_action(state=old_state)             # Accion a realizar; explotando o explorando\n",
    "            new_state, reward, is_final_state = environment.step(next_action)  # Realizamos la accion\n",
    "\n",
    "            learner.update(environment=environment,  # Actualizamos el entorno\n",
    "                           state=deepcopy(old_state),\n",
    "                           action=next_action,\n",
    "                           reward=reward,\n",
    "                           new_state=deepcopy(new_state),\n",
    "                           is_final_state=is_final_state,\n",
    "                           num_episode=n_episode + 1,\n",
    "                           num_steps=num_steps_episode)\n",
    "            num_steps_episode += 1  # Sumamos un paso al episodio\n",
    "\n",
    "        last_episode = {'episode': environment,\n",
    "                        'learner': learner}\n",
    "\n",
    "        if verbose:\n",
    "            # Imprimimos la información de los episodios\n",
    "            print('EPISODIO {} - Numero de acciones: {} - Reward: {}'\n",
    "                  .format(n_episode + 1, num_steps_episode, environment.total_reward))\n",
    "\n",
    "    print_process_info(last_episode=last_episode)\n",
    "\n",
    "\n",
    "def print_process_info(last_episode, print_q_table=True, print_best_values_states=True,\n",
    "                       print_best_actions_states=True, print_steps=True, print_path=True):\n",
    "    \"\"\"\n",
    "    Método que imprime por pantalla los resultados de la ejecución\n",
    "    \"\"\"\n",
    "    if print_q_table:\n",
    "        print('\\nQ_TABLE:')\n",
    "        last_episode['learner'].print_q_table()\n",
    "\n",
    "    if print_best_values_states:\n",
    "        print('\\nBEST Q_TABLE VALUES:')\n",
    "        last_episode['learner'].print_best_values_states()\n",
    "\n",
    "    if print_best_actions_states:\n",
    "        print('\\nBEST ACTIONS:')\n",
    "        last_episode['learner'].print_best_actions_states()\n",
    "\n",
    "    if print_steps:\n",
    "        print('\\nPasos: \\n   {}'.format(last_episode['episode'].actions_done))\n",
    "\n",
    "    if print_path:\n",
    "        print('\\nPATH:')\n",
    "        last_episode['episode'].print_path_episode()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estrategia a corto plazo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODIO 1 - Numero de acciones: 91 - Reward: -391.0\n",
      "EPISODIO 2 - Numero de acciones: 11 - Reward: -11.0\n",
      "EPISODIO 3 - Numero de acciones: 13 - Reward: -213.0\n",
      "EPISODIO 4 - Numero de acciones: 120 - Reward: -3020.0\n",
      "EPISODIO 5 - Numero de acciones: 35 - Reward: 65.0\n",
      "EPISODIO 6 - Numero de acciones: 962 - Reward: -862.0\n",
      "EPISODIO 7 - Numero de acciones: 294 - Reward: -194.0\n",
      "EPISODIO 8 - Numero de acciones: 81 - Reward: -181.0\n",
      "EPISODIO 9 - Numero de acciones: 93 - Reward: -193.0\n",
      "EPISODIO 10 - Numero de acciones: 37 - Reward: 63.0\n",
      "EPISODIO 11 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 12 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 13 - Numero de acciones: 20 - Reward: -620.0\n",
      "EPISODIO 14 - Numero de acciones: 9 - Reward: -9.0\n",
      "EPISODIO 15 - Numero de acciones: 101 - Reward: -101.0\n",
      "EPISODIO 16 - Numero de acciones: 59 - Reward: 41.0\n",
      "EPISODIO 17 - Numero de acciones: 48 - Reward: 52.0\n",
      "EPISODIO 18 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 19 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 20 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 21 - Numero de acciones: 10 - Reward: 90.0\n",
      "EPISODIO 22 - Numero de acciones: 173 - Reward: -573.0\n",
      "EPISODIO 23 - Numero de acciones: 18 - Reward: 82.0\n",
      "EPISODIO 24 - Numero de acciones: 77 - Reward: 23.0\n",
      "EPISODIO 25 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 26 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 27 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 28 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 29 - Numero de acciones: 96 - Reward: -96.0\n",
      "EPISODIO 30 - Numero de acciones: 6 - Reward: 94.0\n",
      "\n",
      "Q_TABLE:\n",
      "Estado  Arriba  Abajo  Izquierda  Derecha\n",
      " x0,y0   -0.59  -0.36      -0.47    -0.32\n",
      " x0,y1   -0.91  -0.12      -0.57    -0.32\n",
      " x0,y2   -1.39   0.08      -0.76    -0.46\n",
      " x0,y3   -1.87   0.27      -0.95    -0.61\n",
      " x1,y0   -0.91  -0.76      -0.82    -0.56\n",
      " x1,y1   -0.93  -0.44      -0.83    -0.33\n",
      " x1,y2   -1.34  -0.05      -0.93    -0.45\n",
      " x1,y3   -1.81   0.19      -1.10    -0.60\n",
      " x2,y0   -1.36  -1.24      -1.25    -1.03\n",
      " x2,y1   -1.27  -1.08      -1.26    -0.69\n",
      " x2,y2   -1.46  -0.59      -1.27    -0.53\n",
      " x2,y3   -1.78  -0.06      -1.30    -0.58\n",
      " x3,y0   -1.80  -1.73      -1.68    -1.49\n",
      " x3,y1   -1.68  -1.57      -1.67    -1.18\n",
      " x3,y2   -1.70  -1.26      -1.69    -0.78\n",
      " x3,y3   -2.00  -0.74      -1.72    -0.76\n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "      y0    y1    y2    y3\n",
      "x0 -0.32 -0.12  0.08  0.27\n",
      "x1 -0.56 -0.33 -0.05  0.19\n",
      "x2 -1.03 -0.69 -0.53 -0.06\n",
      "x3 -1.49 -1.18 -0.78 -0.74\n",
      "\n",
      "BEST ACTIONS:\n",
      "         y0       y1       y2     y3\n",
      "x0  Derecha    Abajo    Abajo  Abajo\n",
      "x1  Derecha  Derecha    Abajo  Abajo\n",
      "x2  Derecha  Derecha  Derecha  Abajo\n",
      "x3  Derecha  Derecha  Derecha  Abajo\n",
      "\n",
      "Pasos: \n",
      "   [[0, 1], [1, 1], [1, 2], [2, 2], [2, 3], [3, 3]]\n",
      "\n",
      "PATH:\n",
      "   y0 y1 y2 y3\n",
      "x0  0  1  -  -\n",
      "x1  -  2  3  -\n",
      "x2  -  -  4  5\n",
      "x3  -  -  -  6\n"
     ]
    }
   ],
   "source": [
    "run_agent(learner=DeepQLearner,\n",
    "          num_episodes=30,\n",
    "          discount_factor=0.1,\n",
    "          ratio_explotacion=0.95,\n",
    "          max_steps=500,\n",
    "          verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estrategia a largo plazo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODIO 1 - Numero de acciones: 16 - Reward: -216.0\n",
      "EPISODIO 2 - Numero de acciones: 140 - Reward: -140.0\n",
      "EPISODIO 3 - Numero de acciones: 38 - Reward: 62.0\n",
      "EPISODIO 4 - Numero de acciones: 154 - Reward: -2854.0\n",
      "EPISODIO 5 - Numero de acciones: 90 - Reward: 10.0\n",
      "EPISODIO 6 - Numero de acciones: 70 - Reward: 30.0\n",
      "EPISODIO 7 - Numero de acciones: 185 - Reward: -85.0\n",
      "EPISODIO 8 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 9 - Numero de acciones: 8 - Reward: -8.0\n",
      "EPISODIO 10 - Numero de acciones: 6 - Reward: -6.0\n",
      "EPISODIO 11 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 12 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 13 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 14 - Numero de acciones: 9 - Reward: 91.0\n",
      "EPISODIO 15 - Numero de acciones: 6 - Reward: -6.0\n",
      "EPISODIO 16 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 17 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 18 - Numero de acciones: 6 - Reward: -6.0\n",
      "EPISODIO 19 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 20 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 21 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 22 - Numero de acciones: 6 - Reward: -6.0\n",
      "EPISODIO 23 - Numero de acciones: 6 - Reward: -6.0\n",
      "EPISODIO 24 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 25 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 26 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 27 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 28 - Numero de acciones: 6 - Reward: -6.0\n",
      "EPISODIO 29 - Numero de acciones: 6 - Reward: -6.0\n",
      "EPISODIO 30 - Numero de acciones: 6 - Reward: 94.0\n",
      "\n",
      "Q_TABLE:\n",
      "Estado  Arriba  Abajo  Izquierda  Derecha\n",
      " x0,y0   -0.20  -0.11      -0.27     0.03\n",
      " x0,y1   -0.54   0.28      -0.87    -0.43\n",
      " x0,y2   -0.82   0.68      -1.45    -0.90\n",
      " x0,y3   -1.11   1.07      -2.03    -1.37\n",
      " x1,y0   -0.56  -0.41      -0.45     0.14\n",
      " x1,y1   -0.86  -0.26      -0.83    -0.06\n",
      " x1,y2   -1.23   0.22      -1.45    -0.55\n",
      " x1,y3   -1.53   0.63      -2.04    -1.03\n",
      " x2,y0   -0.95  -0.65      -0.64     0.16\n",
      " x2,y1   -1.17  -0.72      -0.88     0.19\n",
      " x2,y2   -1.53  -0.36      -1.40    -0.17\n",
      " x2,y3   -1.91   0.15      -2.02    -0.65\n",
      " x3,y0   -1.34  -0.88      -0.83     0.17\n",
      " x3,y1   -1.56  -0.99      -1.05     0.26\n",
      " x3,y2   -1.80  -0.97      -1.35     0.20\n",
      " x3,y3   -2.19  -0.46      -1.97    -0.28\n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "     y0    y1    y2    y3\n",
      "x0 0.03  0.28  0.68  1.07\n",
      "x1 0.14 -0.06  0.22  0.63\n",
      "x2 0.16  0.19 -0.17  0.15\n",
      "x3 0.17  0.26  0.20 -0.28\n",
      "\n",
      "BEST ACTIONS:\n",
      "         y0       y1       y2       y3\n",
      "x0  Derecha    Abajo    Abajo    Abajo\n",
      "x1  Derecha  Derecha    Abajo    Abajo\n",
      "x2  Derecha  Derecha  Derecha    Abajo\n",
      "x3  Derecha  Derecha  Derecha  Derecha\n",
      "\n",
      "Pasos: \n",
      "   [[0, 1], [1, 1], [1, 2], [2, 2], [2, 3], [3, 3]]\n",
      "\n",
      "PATH:\n",
      "   y0 y1 y2 y3\n",
      "x0  0  1  -  -\n",
      "x1  -  2  3  -\n",
      "x2  -  -  4  5\n",
      "x3  -  -  -  6\n"
     ]
    }
   ],
   "source": [
    "run_agent(learner=DeepQLearner,\n",
    "          num_episodes=30,\n",
    "          discount_factor=0.9,\n",
    "          ratio_explotacion=0.95,\n",
    "          max_steps=500,\n",
    "          verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
