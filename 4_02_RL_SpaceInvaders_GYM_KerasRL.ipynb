{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e52b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "# Filtramos los warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Forzar uso CPU en caso de dispones GPU en el PC\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e7e068",
   "metadata": {},
   "source": [
    "# 4.2.- Juego Space Invaders de Open AI GYM (con Keras RL)\n",
    "\n",
    "\n",
    "* GYM de Open AI https://gym.openai.com/: Gym es un conjunto de herramientas para desarrollar y comparar algoritmos de Aprendizaje por Refuerzo (RL). Esta librería ofrece diferentes juegos (entornos) como los juegos de Atari en los que poder probar los algoritmos de RL.\n",
    "\n",
    "\n",
    "* Keras RL https://keras-rl.readthedocs.io/: keras-rl implementa algunos algoritmos de Aprendizaje por Refuerzo en Python y se integra con con Tensorflow-Keras. Además, keras-rl esta listo para su uso con OpenAI Gym, por lo que resulta sencillo evaluar mediante los juegos de GYM el rendimiento de los Algoritmos.\n",
    "\n",
    "<hr>\n",
    "\n",
    "## 1.- Instalación de las librerías (en un Python 3.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b15d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tensorflow==2.6.3 --user\n",
    "!pip3 install keras-rl2 --user\n",
    "!pip3 install atari_py --user\n",
    "!pip3 install gym[atari,accept-rom-license]==0.23.0 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a6074f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2.- Importamos la librería de GYM y atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53de12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import atari_py\n",
    "import gym\n",
    "\n",
    "# Definición de constantes\n",
    "ENVIRONMENT_NAME = \"SpaceInvaders-v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c480dd2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3.- Instanciamos el entorno del Space Invaders y jugamos con un agente que toma acciones aleatorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6161e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciamos el Entorno\n",
    "env = gym.make(ENVIRONMENT_NAME)\n",
    "# env = gym.make(ENVIRONMENT_NAME, render_mode='human')\n",
    "\n",
    "# Obtenemos el tamaño de las imagenes del juego y los canales\n",
    "height, width, n_channels = env.observation_space.shape\n",
    "print('Tamaño de la imagen: {}x{} - Nº Canales: {}'.format(width, height, n_channels))\n",
    "\n",
    "# Vemos las posibles acciones del juego:\n",
    "n_actions = env.action_space.n\n",
    "actions_info = env.unwrapped.get_action_meanings()\n",
    "print('\\nNº de acciones: {} - Acciones: {}\\n'.format(n_actions, actions_info))\n",
    "\n",
    "# Jugamos 2 partidas de manera aleatoria\n",
    "episodes = 2\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()  # Inicializamos el entorno\n",
    "    done = False         # Flag de finalización de la partida (episodio)\n",
    "    score = 0            # Contador de recompensas\n",
    "    \n",
    "    # ! A Jugar ¡ (Hasta que termine la partida -> done == True)\n",
    "    while not done:\n",
    "        action = env.action_space.sample()              # Seleccionamos una acción Aleatoria\n",
    "        n_state, reward, done, info = env.step(action)  # Realizamos la acción aleatoria y obtenemos: \n",
    "                                                        # 1. Lista de estados, 2. Recompensa, 3. ¿Fin del juego?, 4. info\n",
    "        score+=reward                                   # Sumamos la recompensa de la acción\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b18fc64",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4.- Creamos un modelo de red neuronal convolucional\n",
    "\n",
    "\n",
    "* Para tomar las acciones vamos a usar una red neuronal convolucional que:\n",
    "\n",
    "    + Tomará como entrada la imagen de la partida (Alto de 210px, Ancho de 160px y 3 canales)\n",
    "    + La capa de salida tendrá 6 Neuronas, 1 por posible acción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660b199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=32, \n",
    "                            kernel_size=(8,8), \n",
    "                            strides=(4,4), \n",
    "                            activation='relu', \n",
    "                            input_shape=(3, height, width, n_channels)))\n",
    "    model.add(Convolution2D(filters=64, \n",
    "                            kernel_size=(4,4), \n",
    "                            strides=(2,2), \n",
    "                            activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(n_actions, activation='linear'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_cnn_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea0730",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 5.- Construimos el agente con Keral RL y lo entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8730ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "\n",
    "model = build_cnn_model()\n",
    "memory = SequentialMemory(limit=1000, window_length=3)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
    "dqn = DQNAgent(model=model, \n",
    "               memory=memory, \n",
    "               policy=policy,\n",
    "               enable_dueling_network=True, \n",
    "               nb_actions=n_actions, \n",
    "               nb_steps_warmup=100)\n",
    "\n",
    "# Compilamos el modelo\n",
    "dqn.compile(Adam(lr=1e-4))\n",
    "\n",
    "# Entrenamos el modelo\n",
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05353273",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 6.- Explotamos el modelo. Nos ponemos a jugar con el agente ya entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0458db49",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = gym.make(ENVIRONMENT_NAME, render_mode='human', )\n",
    "scores = dqn.test(env_test, nb_episodes=5, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
