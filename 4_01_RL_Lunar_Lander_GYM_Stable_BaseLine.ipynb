{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69cbd648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "# Filtramos los warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Forzar uso CPU en caso de dispones GPU en el PC\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf71c0e",
   "metadata": {},
   "source": [
    "# 4.1.- Juego Lunar Lander de Open AI GYM (con Stable Baseline)\n",
    "\n",
    "\n",
    "* GYM de Open AI https://gym.openai.com/: Gym es un conjunto de herramientas para desarrollar y comparar algoritmos de Aprendizaje por Refuerzo (RL). Esta librería ofrece diferentes juegos (entornos) como los juegos de Atari en los que poder probar los algoritmos de RL.\n",
    "\n",
    "\n",
    "* Stable Baseline https://stable-baselines.readthedocs.io/: Librería en python en la que tiene implementados diferentes algoritmos y funcionalidades de Aprendizaje por Refuerzo.\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "## 1.- Instalación de las librerías (en un Python 3.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6df48f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tensorflow==1.15.0\n",
    "!pip3 install stable_baselines\n",
    "!pip3 install box2d-py\n",
    "!pip3 install gym[atari]==0.19.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11094699",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2.- Importamos las librerías de GYM y Stable Baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d4b21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym \n",
    "\n",
    "from stable_baselines import ACER\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "\n",
    "# Definición de constantes\n",
    "ENVIRONMENT_NAME = 'LunarLander-v2'\n",
    "FILE_MODEL_NAME = \"ACER_LunarLander_Model\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eacb5cd",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3.- Instanciamos el entorno del \"Lunar Lander\" y realizamos acciones aleatorias\n",
    "\n",
    "\n",
    "* Lunar Lander es un sencillo juego que consiste en aterrizar una nave entre dos banderas.\n",
    "\n",
    "* En este juego se pueden realizar 4 ***acciones*** que son:\n",
    "\n",
    "    + no hacer nada            (acción 0)\n",
    "    + activar motor izquierdo  (acción 1)\n",
    "    + activar motor de abajo   (acción 2)\n",
    "    + activar motor derecho    (acción 3)\n",
    "    \n",
    "\n",
    "* Las ***observaciones*** estan definidas por ***8 estados***: las coordenadas del módulo de aterrizaje en x & y, su lineal velocidades en x & y, su ángulo, su velocidad angular y dos valores booleanos que representan si cada pierna está en contacto con el suelo o no.\n",
    "\n",
    "\n",
    "* ***Estado inicial***: La nave comienza en la parte superior central de la pantalla con una fuerza inicial aleatoria aplicada a su centro de masa.\n",
    "\n",
    "\n",
    "* ***Recompensas***: \n",
    "\n",
    "    + Recompensa por moverse desde la parte superior de la pantalla a la plataforma de aterrizaje y venir para descansar es de unos 100-140 puntos.\n",
    "    + Si el módulo de aterrizaje se aleja de la plataforma de aterrizaje, pierde la recompensa.\n",
    "    + Si el módulo de aterrizaje se estrella, recibe -100 puntos adicionales. \n",
    "    + Cada pata de la nave en contacto con el suelo es +10 puntos.\n",
    "    + Activar el motor principal es -0.3 puntos. \n",
    "    + Activar el motor lateral es -0.03 puntos. \n",
    "    + Resuelto son 200 puntos.\n",
    "    \n",
    "    \n",
    "* ***Fin del episodio***: La partida termina en alguno de los 3 casos:\n",
    "\n",
    "    + la nave se estrella (el cuerpo de la nave entra en contacto con la luna).\n",
    "    + la nave sale de la ventana gráfica.\n",
    "    + la nave se queda quieta (no se mueve)\n",
    "\n",
    "\n",
    "* Código: https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02a574a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acciones del Juego: 4\n",
      "Episode:1 Score:-225.53990591703925\n",
      "Episode:2 Score:-102.87568725927923\n",
      "Episode:3 Score:-79.17159308892002\n",
      "Episode:4 Score:-86.45601192382098\n",
      "Episode:5 Score:-226.69039410632342\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENVIRONMENT_NAME)\n",
    "\n",
    "print('Acciones del Juego: {}'.format(env.action_space.n))\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()  # Inicializamos el entorno\n",
    "    done = False         # Flag de finalización de la partida (episodio)\n",
    "    score = 0            # Contador de recompensas\n",
    "    \n",
    "    # ! A Jugar ¡ (Hasta que termine la partida -> done == True)\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()              # Seleccionamos una acción Aleatoria\n",
    "        n_state, reward, done, info = env.step(action)  # Realizamos la acción aleatoria y obtenemos: \n",
    "                                                        # 1. Lista de estados, 2. Recompensa, 3. ¿Fin del juego?, 4. info\n",
    "        score+=reward                                   # Sumamos la recompensa de la acción\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9753a9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4.- Entrenamos un agente que aprenda a aterrizar la nave entre las banderas y exportamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "871e1b88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python37\\site-packages\\stable_baselines\\common\\tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python37\\site-packages\\stable_baselines\\common\\tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python37\\site-packages\\stable_baselines\\common\\policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python37\\site-packages\\stable_baselines\\common\\input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python37\\site-packages\\stable_baselines\\common\\policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\layers\\core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python37\\site-packages\\stable_baselines\\common\\tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python37\\site-packages\\stable_baselines\\common\\distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python37\\site-packages\\stable_baselines\\common\\distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python37\\site-packages\\stable_baselines\\common\\tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python37\\site-packages\\stable_baselines\\common\\tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\training\\moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python37\\site-packages\\stable_baselines\\acer\\acer_simple.py:421: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python37\\site-packages\\stable_baselines\\acer\\acer_simple.py:453: The name tf.global_norm is deprecated. Please use tf.linalg.global_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python37\\site-packages\\stable_baselines\\acer\\acer_simple.py:479: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python37\\site-packages\\stable_baselines\\acer\\acer_simple.py:504: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python37\\site-packages\\stable_baselines\\acer\\acer_simple.py:506: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 6.63     |\n",
      "| avg_norm_g          | 27.5     |\n",
      "| avg_norm_grads_f    | 23.8     |\n",
      "| avg_norm_k          | 2        |\n",
      "| avg_norm_k_dot_g    | 27.5     |\n",
      "| entropy             | 29.1     |\n",
      "| explained_variance  | 7.21e-06 |\n",
      "| fps                 | 0        |\n",
      "| loss                | 3.88     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -9.54    |\n",
      "| loss_policy         | -9.54    |\n",
      "| loss_q              | 27.4     |\n",
      "| mean_episode_length | 0        |\n",
      "| mean_episode_reward | 0        |\n",
      "| norm_grads          | 6.3      |\n",
      "| norm_grads_policy   | 3.76     |\n",
      "| norm_grads_q        | 5.06     |\n",
      "| total_timesteps     | 20       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 54.7     |\n",
      "| avg_norm_g          | 225      |\n",
      "| avg_norm_grads_f    | 196      |\n",
      "| avg_norm_k          | 2.01     |\n",
      "| avg_norm_k_dot_g    | 225      |\n",
      "| entropy             | 28.9     |\n",
      "| explained_variance  | -0.0211  |\n",
      "| fps                 | 930      |\n",
      "| loss                | 1.86e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -77.6    |\n",
      "| loss_policy         | -77.6    |\n",
      "| loss_q              | 3.87e+03 |\n",
      "| mean_episode_length | 95.6     |\n",
      "| mean_episode_reward | -180     |\n",
      "| norm_grads          | 92.6     |\n",
      "| norm_grads_policy   | 28       |\n",
      "| norm_grads_q        | 88.3     |\n",
      "| total_timesteps     | 2020     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 6.57     |\n",
      "| avg_norm_g          | 27.5     |\n",
      "| avg_norm_grads_f    | 23.8     |\n",
      "| avg_norm_k          | 2.03     |\n",
      "| avg_norm_k_dot_g    | 28       |\n",
      "| entropy             | 28.4     |\n",
      "| explained_variance  | 0.0877   |\n",
      "| fps                 | 903      |\n",
      "| loss                | 2.95     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -9.21    |\n",
      "| loss_policy         | -9.21    |\n",
      "| loss_q              | 24.9     |\n",
      "| mean_episode_length | 102      |\n",
      "| mean_episode_reward | -207     |\n",
      "| norm_grads          | 13.4     |\n",
      "| norm_grads_policy   | 2.52     |\n",
      "| norm_grads_q        | 13.2     |\n",
      "| total_timesteps     | 4020     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 1.7      |\n",
      "| avg_norm_g          | 11.1     |\n",
      "| avg_norm_grads_f    | 10.1     |\n",
      "| avg_norm_k          | 2.22     |\n",
      "| avg_norm_k_dot_g    | 12.1     |\n",
      "| entropy             | 21.5     |\n",
      "| explained_variance  | 0.126    |\n",
      "| fps                 | 880      |\n",
      "| loss                | 0.328    |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -1.17    |\n",
      "| loss_policy         | -1.17    |\n",
      "| loss_q              | 3.42     |\n",
      "| mean_episode_length | 119      |\n",
      "| mean_episode_reward | -261     |\n",
      "| norm_grads          | 2.49     |\n",
      "| norm_grads_policy   | 1.99     |\n",
      "| norm_grads_q        | 1.5      |\n",
      "| total_timesteps     | 6020     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 226      |\n",
      "| avg_norm_g          | 915      |\n",
      "| avg_norm_grads_f    | 627      |\n",
      "| avg_norm_k          | 2.91     |\n",
      "| avg_norm_k_dot_g    | 1.92e+03 |\n",
      "| entropy             | 15.2     |\n",
      "| explained_variance  | 0.00954  |\n",
      "| fps                 | 863      |\n",
      "| loss                | 187      |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -20.1    |\n",
      "| loss_policy         | -20.1    |\n",
      "| loss_q              | 415      |\n",
      "| mean_episode_length | 144      |\n",
      "| mean_episode_reward | -281     |\n",
      "| norm_grads          | 218      |\n",
      "| norm_grads_policy   | 208      |\n",
      "| norm_grads_q        | 66.7     |\n",
      "| total_timesteps     | 8020     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| avg_norm_adj        | 0.0224   |\n",
      "| avg_norm_g          | 25.8     |\n",
      "| avg_norm_grads_f    | 25.7     |\n",
      "| avg_norm_k          | 2.63     |\n",
      "| avg_norm_k_dot_g    | 36.5     |\n",
      "| entropy             | 15.2     |\n",
      "| explained_variance  | -0.0874  |\n",
      "| fps                 | 837      |\n",
      "| loss                | 12.2     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 3.98     |\n",
      "| loss_policy         | 3.98     |\n",
      "| loss_q              | 16.8     |\n",
      "| mean_episode_length | 173      |\n",
      "| mean_episode_reward | -263     |\n",
      "| norm_grads          | 14.6     |\n",
      "| norm_grads_policy   | 1.21     |\n",
      "| norm_grads_q        | 14.5     |\n",
      "| total_timesteps     | 10020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 1.99     |\n",
      "| avg_norm_g          | 24.2     |\n",
      "| avg_norm_grads_f    | 23.2     |\n",
      "| avg_norm_k          | 1.77     |\n",
      "| avg_norm_k_dot_g    | 21.7     |\n",
      "| entropy             | 23       |\n",
      "| explained_variance  | 0.0825   |\n",
      "| fps                 | 817      |\n",
      "| loss                | 23.5     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 4.44     |\n",
      "| loss_policy         | 4.44     |\n",
      "| loss_q              | 38.6     |\n",
      "| mean_episode_length | 188      |\n",
      "| mean_episode_reward | -257     |\n",
      "| norm_grads          | 11.4     |\n",
      "| norm_grads_policy   | 6.13     |\n",
      "| norm_grads_q        | 9.63     |\n",
      "| total_timesteps     | 12020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0.596    |\n",
      "| avg_norm_g          | 7.63     |\n",
      "| avg_norm_grads_f    | 7.16     |\n",
      "| avg_norm_k          | 2.03     |\n",
      "| avg_norm_k_dot_g    | 7.88     |\n",
      "| entropy             | 27.2     |\n",
      "| explained_variance  | 0.896    |\n",
      "| fps                 | 758      |\n",
      "| loss                | 1.69     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 0.709    |\n",
      "| loss_policy         | 0.709    |\n",
      "| loss_q              | 2.5      |\n",
      "| mean_episode_length | 237      |\n",
      "| mean_episode_reward | -243     |\n",
      "| norm_grads          | 5.14     |\n",
      "| norm_grads_policy   | 1.21     |\n",
      "| norm_grads_q        | 5        |\n",
      "| total_timesteps     | 14020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 5.72     |\n",
      "| avg_norm_g          | 27.6     |\n",
      "| avg_norm_grads_f    | 24.4     |\n",
      "| avg_norm_k          | 2.17     |\n",
      "| avg_norm_k_dot_g    | 28.2     |\n",
      "| entropy             | 8.99     |\n",
      "| explained_variance  | 0.0324   |\n",
      "| fps                 | 724      |\n",
      "| loss                | 28.6     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -2.7     |\n",
      "| loss_policy         | -2.7     |\n",
      "| loss_q              | 62.8     |\n",
      "| mean_episode_length | 279      |\n",
      "| mean_episode_reward | -220     |\n",
      "| norm_grads          | 124      |\n",
      "| norm_grads_policy   | 8.33     |\n",
      "| norm_grads_q        | 123      |\n",
      "| total_timesteps     | 16020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0.158    |\n",
      "| avg_norm_g          | 1.54     |\n",
      "| avg_norm_grads_f    | 1.41     |\n",
      "| avg_norm_k          | 2.2      |\n",
      "| avg_norm_k_dot_g    | 1.56     |\n",
      "| entropy             | 22.2     |\n",
      "| explained_variance  | 0.0269   |\n",
      "| fps                 | 706      |\n",
      "| loss                | -0.66    |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -0.514   |\n",
      "| loss_policy         | -0.514   |\n",
      "| loss_q              | 0.153    |\n",
      "| mean_episode_length | 311      |\n",
      "| mean_episode_reward | -209     |\n",
      "| norm_grads          | 8.65     |\n",
      "| norm_grads_policy   | 0.559    |\n",
      "| norm_grads_q        | 8.64     |\n",
      "| total_timesteps     | 18020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0.821    |\n",
      "| avg_norm_g          | 11.8     |\n",
      "| avg_norm_grads_f    | 11.4     |\n",
      "| avg_norm_k          | 2.07     |\n",
      "| avg_norm_k_dot_g    | 12.7     |\n",
      "| entropy             | 20.6     |\n",
      "| explained_variance  | 0.0253   |\n",
      "| fps                 | 709      |\n",
      "| loss                | 6.52     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 1.8      |\n",
      "| loss_policy         | 1.8      |\n",
      "| loss_q              | 9.85     |\n",
      "| mean_episode_length | 359      |\n",
      "| mean_episode_reward | -168     |\n",
      "| norm_grads          | 56.6     |\n",
      "| norm_grads_policy   | 6.69     |\n",
      "| norm_grads_q        | 56.2     |\n",
      "| total_timesteps     | 20020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0        |\n",
      "| avg_norm_g          | 0.752    |\n",
      "| avg_norm_grads_f    | 0.752    |\n",
      "| avg_norm_k          | 1.83     |\n",
      "| avg_norm_k_dot_g    | 0.752    |\n",
      "| entropy             | 18       |\n",
      "| explained_variance  | 0.0492   |\n",
      "| fps                 | 707      |\n",
      "| loss                | 0.0242   |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 0.149    |\n",
      "| loss_policy         | 0.149    |\n",
      "| loss_q              | 0.111    |\n",
      "| mean_episode_length | 384      |\n",
      "| mean_episode_reward | -132     |\n",
      "| norm_grads          | 5.25     |\n",
      "| norm_grads_policy   | 0.198    |\n",
      "| norm_grads_q        | 5.25     |\n",
      "| total_timesteps     | 22020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 5.1      |\n",
      "| avg_norm_g          | 39.3     |\n",
      "| avg_norm_grads_f    | 29.1     |\n",
      "| avg_norm_k          | 4.08     |\n",
      "| avg_norm_k_dot_g    | 109      |\n",
      "| entropy             | 3.18     |\n",
      "| explained_variance  | 0.0673   |\n",
      "| fps                 | 701      |\n",
      "| loss                | 28.5     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -1.83    |\n",
      "| loss_policy         | -1.83    |\n",
      "| loss_q              | 60.7     |\n",
      "| mean_episode_length | 423      |\n",
      "| mean_episode_reward | -99.8    |\n",
      "| norm_grads          | 25.4     |\n",
      "| norm_grads_policy   | 13.5     |\n",
      "| norm_grads_q        | 21.5     |\n",
      "| total_timesteps     | 24020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0        |\n",
      "| avg_norm_g          | 25.6     |\n",
      "| avg_norm_grads_f    | 25.6     |\n",
      "| avg_norm_k          | 2.18     |\n",
      "| avg_norm_k_dot_g    | 28.6     |\n",
      "| entropy             | 11.6     |\n",
      "| explained_variance  | 0.138    |\n",
      "| fps                 | 709      |\n",
      "| loss                | 51.7     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 7.52     |\n",
      "| loss_policy         | 7.52     |\n",
      "| loss_q              | 88.7     |\n",
      "| mean_episode_length | 438      |\n",
      "| mean_episode_reward | -64.8    |\n",
      "| norm_grads          | 218      |\n",
      "| norm_grads_policy   | 16.6     |\n",
      "| norm_grads_q        | 218      |\n",
      "| total_timesteps     | 26020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 1.7      |\n",
      "| avg_norm_g          | 7.78     |\n",
      "| avg_norm_grads_f    | 6.78     |\n",
      "| avg_norm_k          | 1.97     |\n",
      "| avg_norm_k_dot_g    | 7.57     |\n",
      "| entropy             | 15       |\n",
      "| explained_variance  | -0.119   |\n",
      "| fps                 | 701      |\n",
      "| loss                | -0.403   |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -1.87    |\n",
      "| loss_policy         | -1.87    |\n",
      "| loss_q              | 3.23     |\n",
      "| mean_episode_length | 462      |\n",
      "| mean_episode_reward | -52      |\n",
      "| norm_grads          | 40.9     |\n",
      "| norm_grads_policy   | 2.29     |\n",
      "| norm_grads_q        | 40.9     |\n",
      "| total_timesteps     | 28020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0        |\n",
      "| avg_norm_g          | 0.223    |\n",
      "| avg_norm_grads_f    | 0.223    |\n",
      "| avg_norm_k          | 2.27     |\n",
      "| avg_norm_k_dot_g    | 0.22     |\n",
      "| entropy             | 9.9      |\n",
      "| explained_variance  | -0.197   |\n",
      "| fps                 | 696      |\n",
      "| loss                | -0.0674  |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 0.0203   |\n",
      "| loss_policy         | 0.0203   |\n",
      "| loss_q              | 0.0226   |\n",
      "| mean_episode_length | 495      |\n",
      "| mean_episode_reward | -19.8    |\n",
      "| norm_grads          | 1.5      |\n",
      "| norm_grads_policy   | 0.0598   |\n",
      "| norm_grads_q        | 1.5      |\n",
      "| total_timesteps     | 30020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0        |\n",
      "| avg_norm_g          | 27.5     |\n",
      "| avg_norm_grads_f    | 27.5     |\n",
      "| avg_norm_k          | 1.95     |\n",
      "| avg_norm_k_dot_g    | 27.2     |\n",
      "| entropy             | 13       |\n",
      "| explained_variance  | -0.0623  |\n",
      "| fps                 | 692      |\n",
      "| loss                | 75.7     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 8.4      |\n",
      "| loss_policy         | 8.4      |\n",
      "| loss_q              | 135      |\n",
      "| mean_episode_length | 523      |\n",
      "| mean_episode_reward | -0.519   |\n",
      "| norm_grads          | 262      |\n",
      "| norm_grads_policy   | 20.3     |\n",
      "| norm_grads_q        | 261      |\n",
      "| total_timesteps     | 32020    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| avg_norm_adj        | 35.4     |\n",
      "| avg_norm_g          | 185      |\n",
      "| avg_norm_grads_f    | 167      |\n",
      "| avg_norm_k          | 2.35     |\n",
      "| avg_norm_k_dot_g    | 178      |\n",
      "| entropy             | 14.1     |\n",
      "| explained_variance  | -0.585   |\n",
      "| fps                 | 698      |\n",
      "| loss                | 1.55e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -45.1    |\n",
      "| loss_policy         | -45.1    |\n",
      "| loss_q              | 3.19e+03 |\n",
      "| mean_episode_length | 455      |\n",
      "| mean_episode_reward | 5.31     |\n",
      "| norm_grads          | 1.3e+03  |\n",
      "| norm_grads_policy   | 148      |\n",
      "| norm_grads_q        | 1.29e+03 |\n",
      "| total_timesteps     | 34020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0        |\n",
      "| avg_norm_g          | 86.1     |\n",
      "| avg_norm_grads_f    | 86.1     |\n",
      "| avg_norm_k          | 1.85     |\n",
      "| avg_norm_k_dot_g    | 79.7     |\n",
      "| entropy             | 11.5     |\n",
      "| explained_variance  | 0.368    |\n",
      "| fps                 | 703      |\n",
      "| loss                | 445      |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 25       |\n",
      "| loss_policy         | 25       |\n",
      "| loss_q              | 840      |\n",
      "| mean_episode_length | 383      |\n",
      "| mean_episode_reward | 16.1     |\n",
      "| norm_grads          | 251      |\n",
      "| norm_grads_policy   | 94       |\n",
      "| norm_grads_q        | 232      |\n",
      "| total_timesteps     | 36020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0.0191   |\n",
      "| avg_norm_g          | 511      |\n",
      "| avg_norm_grads_f    | 511      |\n",
      "| avg_norm_k          | 1.8      |\n",
      "| avg_norm_k_dot_g    | 443      |\n",
      "| entropy             | 12.5     |\n",
      "| explained_variance  | 0.0896   |\n",
      "| fps                 | 707      |\n",
      "| loss                | 40       |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 10.7     |\n",
      "| loss_policy         | 10.7     |\n",
      "| loss_q              | 58.8     |\n",
      "| mean_episode_length | 317      |\n",
      "| mean_episode_reward | -2.32    |\n",
      "| norm_grads          | 276      |\n",
      "| norm_grads_policy   | 22.6     |\n",
      "| norm_grads_q        | 276      |\n",
      "| total_timesteps     | 38020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0        |\n",
      "| avg_norm_g          | 32.4     |\n",
      "| avg_norm_grads_f    | 32.4     |\n",
      "| avg_norm_k          | 1.79     |\n",
      "| avg_norm_k_dot_g    | 30.8     |\n",
      "| entropy             | 12.5     |\n",
      "| explained_variance  | -0.168   |\n",
      "| fps                 | 712      |\n",
      "| loss                | 133      |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 8.98     |\n",
      "| loss_policy         | 8.98     |\n",
      "| loss_q              | 249      |\n",
      "| mean_episode_length | 258      |\n",
      "| mean_episode_reward | -6.58    |\n",
      "| norm_grads          | 92.1     |\n",
      "| norm_grads_policy   | 21.3     |\n",
      "| norm_grads_q        | 89.6     |\n",
      "| total_timesteps     | 40020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 3.34     |\n",
      "| avg_norm_g          | 29.8     |\n",
      "| avg_norm_grads_f    | 26.4     |\n",
      "| avg_norm_k          | 3.16     |\n",
      "| avg_norm_k_dot_g    | 41.3     |\n",
      "| entropy             | 13.5     |\n",
      "| explained_variance  | -0.329   |\n",
      "| fps                 | 712      |\n",
      "| loss                | 88.4     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -7.25    |\n",
      "| loss_policy         | -7.25    |\n",
      "| loss_q              | 191      |\n",
      "| mean_episode_length | 228      |\n",
      "| mean_episode_reward | -18.1    |\n",
      "| norm_grads          | 347      |\n",
      "| norm_grads_policy   | 20.2     |\n",
      "| norm_grads_q        | 346      |\n",
      "| total_timesteps     | 42020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0.0308   |\n",
      "| avg_norm_g          | 23.8     |\n",
      "| avg_norm_grads_f    | 23.8     |\n",
      "| avg_norm_k          | 1.96     |\n",
      "| avg_norm_k_dot_g    | 23.4     |\n",
      "| entropy             | 15.6     |\n",
      "| explained_variance  | 0.401    |\n",
      "| fps                 | 716      |\n",
      "| loss                | 9.78     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 3.57     |\n",
      "| loss_policy         | 3.57     |\n",
      "| loss_q              | 12.7     |\n",
      "| mean_episode_length | 212      |\n",
      "| mean_episode_reward | -26.7    |\n",
      "| norm_grads          | 118      |\n",
      "| norm_grads_policy   | 12.1     |\n",
      "| norm_grads_q        | 117      |\n",
      "| total_timesteps     | 44020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 67.1     |\n",
      "| avg_norm_g          | 175      |\n",
      "| avg_norm_grads_f    | 136      |\n",
      "| avg_norm_k          | 1.58     |\n",
      "| avg_norm_k_dot_g    | 178      |\n",
      "| entropy             | 4.15     |\n",
      "| explained_variance  | -0.352   |\n",
      "| fps                 | 716      |\n",
      "| loss                | 5.43e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -22.4    |\n",
      "| loss_policy         | -22.4    |\n",
      "| loss_q              | 1.09e+04 |\n",
      "| mean_episode_length | 246      |\n",
      "| mean_episode_reward | -9.73    |\n",
      "| norm_grads          | 1.25e+03 |\n",
      "| norm_grads_policy   | 104      |\n",
      "| norm_grads_q        | 1.25e+03 |\n",
      "| total_timesteps     | 46020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0.1      |\n",
      "| avg_norm_g          | 0.873    |\n",
      "| avg_norm_grads_f    | 0.794    |\n",
      "| avg_norm_k          | 2.13     |\n",
      "| avg_norm_k_dot_g    | 0.931    |\n",
      "| entropy             | 12.1     |\n",
      "| explained_variance  | 0.0849   |\n",
      "| fps                 | 715      |\n",
      "| loss                | -0.288   |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -0.192   |\n",
      "| loss_policy         | -0.192   |\n",
      "| loss_q              | 0.05     |\n",
      "| mean_episode_length | 280      |\n",
      "| mean_episode_reward | 1.34     |\n",
      "| norm_grads          | 6.75     |\n",
      "| norm_grads_policy   | 0.451    |\n",
      "| norm_grads_q        | 6.73     |\n",
      "| total_timesteps     | 48020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0.505    |\n",
      "| avg_norm_g          | 3.07     |\n",
      "| avg_norm_grads_f    | 2.57     |\n",
      "| avg_norm_k          | 2.63     |\n",
      "| avg_norm_k_dot_g    | 4.22     |\n",
      "| entropy             | 11.7     |\n",
      "| explained_variance  | -0.0588  |\n",
      "| fps                 | 714      |\n",
      "| loss                | -0.376   |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -0.888   |\n",
      "| loss_policy         | -0.888   |\n",
      "| loss_q              | 1.26     |\n",
      "| mean_episode_length | 310      |\n",
      "| mean_episode_reward | 20.8     |\n",
      "| norm_grads          | 31.8     |\n",
      "| norm_grads_policy   | 1.61     |\n",
      "| norm_grads_q        | 31.7     |\n",
      "| total_timesteps     | 50020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0.0369   |\n",
      "| avg_norm_g          | 3.28     |\n",
      "| avg_norm_grads_f    | 3.25     |\n",
      "| avg_norm_k          | 1.84     |\n",
      "| avg_norm_k_dot_g    | 2.86     |\n",
      "| entropy             | 15.5     |\n",
      "| explained_variance  | 0.883    |\n",
      "| fps                 | 714      |\n",
      "| loss                | 0.674    |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 0.525    |\n",
      "| loss_policy         | 0.525    |\n",
      "| loss_q              | 0.609    |\n",
      "| mean_episode_length | 337      |\n",
      "| mean_episode_reward | 44.2     |\n",
      "| norm_grads          | 8.64     |\n",
      "| norm_grads_policy   | 2.02     |\n",
      "| norm_grads_q        | 8.4      |\n",
      "| total_timesteps     | 52020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0.017    |\n",
      "| avg_norm_g          | 0.844    |\n",
      "| avg_norm_grads_f    | 0.83     |\n",
      "| avg_norm_k          | 2.17     |\n",
      "| avg_norm_k_dot_g    | 0.843    |\n",
      "| entropy             | 14.7     |\n",
      "| explained_variance  | 0.0642   |\n",
      "| fps                 | 714      |\n",
      "| loss                | -0.0379  |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 0.0411   |\n",
      "| loss_policy         | 0.0411   |\n",
      "| loss_q              | 0.136    |\n",
      "| mean_episode_length | 356      |\n",
      "| mean_episode_reward | 57.7     |\n",
      "| norm_grads          | 3.39     |\n",
      "| norm_grads_policy   | 0.995    |\n",
      "| norm_grads_q        | 3.25     |\n",
      "| total_timesteps     | 54020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 2.18     |\n",
      "| avg_norm_g          | 9.59     |\n",
      "| avg_norm_grads_f    | 8.41     |\n",
      "| avg_norm_k          | 1.81     |\n",
      "| avg_norm_k_dot_g    | 9.48     |\n",
      "| entropy             | 6.67     |\n",
      "| explained_variance  | 0.311    |\n",
      "| fps                 | 709      |\n",
      "| loss                | 220      |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -1.37    |\n",
      "| loss_policy         | -1.37    |\n",
      "| loss_q              | 442      |\n",
      "| mean_episode_length | 372      |\n",
      "| mean_episode_reward | 57.1     |\n",
      "| norm_grads          | 185      |\n",
      "| norm_grads_policy   | 22.7     |\n",
      "| norm_grads_q        | 184      |\n",
      "| total_timesteps     | 56020    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| avg_norm_adj        | 39.3     |\n",
      "| avg_norm_g          | 150      |\n",
      "| avg_norm_grads_f    | 130      |\n",
      "| avg_norm_k          | 1.68     |\n",
      "| avg_norm_k_dot_g    | 142      |\n",
      "| entropy             | 9.4      |\n",
      "| explained_variance  | -2.15    |\n",
      "| fps                 | 706      |\n",
      "| loss                | 237      |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -26.8    |\n",
      "| loss_policy         | -26.8    |\n",
      "| loss_q              | 527      |\n",
      "| mean_episode_length | 404      |\n",
      "| mean_episode_reward | 71.2     |\n",
      "| norm_grads          | 455      |\n",
      "| norm_grads_policy   | 28.1     |\n",
      "| norm_grads_q        | 454      |\n",
      "| total_timesteps     | 58020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 1.39     |\n",
      "| avg_norm_g          | 8.12     |\n",
      "| avg_norm_grads_f    | 7.21     |\n",
      "| avg_norm_k          | 2.42     |\n",
      "| avg_norm_k_dot_g    | 9.09     |\n",
      "| entropy             | 14.6     |\n",
      "| explained_variance  | 0.773    |\n",
      "| fps                 | 705      |\n",
      "| loss                | -0.266   |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -1.94    |\n",
      "| loss_policy         | -1.94    |\n",
      "| loss_q              | 3.63     |\n",
      "| mean_episode_length | 429      |\n",
      "| mean_episode_reward | 80.9     |\n",
      "| norm_grads          | 34.1     |\n",
      "| norm_grads_policy   | 5.12     |\n",
      "| norm_grads_q        | 33.7     |\n",
      "| total_timesteps     | 60020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 4.57     |\n",
      "| avg_norm_g          | 17.7     |\n",
      "| avg_norm_grads_f    | 15.1     |\n",
      "| avg_norm_k          | 1.93     |\n",
      "| avg_norm_k_dot_g    | 17.6     |\n",
      "| entropy             | 21.1     |\n",
      "| explained_variance  | 0.48     |\n",
      "| fps                 | 706      |\n",
      "| loss                | 2.75     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -5.18    |\n",
      "| loss_policy         | -5.18    |\n",
      "| loss_q              | 16.3     |\n",
      "| mean_episode_length | 403      |\n",
      "| mean_episode_reward | 77.1     |\n",
      "| norm_grads          | 136      |\n",
      "| norm_grads_policy   | 15.4     |\n",
      "| norm_grads_q        | 135      |\n",
      "| total_timesteps     | 62020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0        |\n",
      "| avg_norm_g          | 3.83     |\n",
      "| avg_norm_grads_f    | 3.83     |\n",
      "| avg_norm_k          | 2.31     |\n",
      "| avg_norm_k_dot_g    | 4.03     |\n",
      "| entropy             | 13.5     |\n",
      "| explained_variance  | 0.33     |\n",
      "| fps                 | 706      |\n",
      "| loss                | 1.04     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 0.781    |\n",
      "| loss_policy         | 0.781    |\n",
      "| loss_q              | 0.796    |\n",
      "| mean_episode_length | 385      |\n",
      "| mean_episode_reward | 87.7     |\n",
      "| norm_grads          | 30.6     |\n",
      "| norm_grads_policy   | 0.367    |\n",
      "| norm_grads_q        | 30.6     |\n",
      "| total_timesteps     | 64020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0        |\n",
      "| avg_norm_g          | 0.888    |\n",
      "| avg_norm_grads_f    | 0.888    |\n",
      "| avg_norm_k          | 1.91     |\n",
      "| avg_norm_k_dot_g    | 0.567    |\n",
      "| entropy             | 6.39     |\n",
      "| explained_variance  | 0.376    |\n",
      "| fps                 | 708      |\n",
      "| loss                | 0.0887   |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 0.131    |\n",
      "| loss_policy         | 0.131    |\n",
      "| loss_q              | 0.0426   |\n",
      "| mean_episode_length | 345      |\n",
      "| mean_episode_reward | 81.2     |\n",
      "| norm_grads          | 1.84     |\n",
      "| norm_grads_policy   | 1.43     |\n",
      "| norm_grads_q        | 1.16     |\n",
      "| total_timesteps     | 66020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0.00426  |\n",
      "| avg_norm_g          | 10       |\n",
      "| avg_norm_grads_f    | 10       |\n",
      "| avg_norm_k          | 1.88     |\n",
      "| avg_norm_k_dot_g    | 9.56     |\n",
      "| entropy             | 15.5     |\n",
      "| explained_variance  | 0.523    |\n",
      "| fps                 | 711      |\n",
      "| loss                | 7.29     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 2.49     |\n",
      "| loss_policy         | 2.49     |\n",
      "| loss_q              | 9.93     |\n",
      "| mean_episode_length | 292      |\n",
      "| mean_episode_reward | 57.7     |\n",
      "| norm_grads          | 58.8     |\n",
      "| norm_grads_policy   | 5.28     |\n",
      "| norm_grads_q        | 58.6     |\n",
      "| total_timesteps     | 68020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0.0361   |\n",
      "| avg_norm_g          | 2        |\n",
      "| avg_norm_grads_f    | 1.94     |\n",
      "| avg_norm_k          | 3.26     |\n",
      "| avg_norm_k_dot_g    | 3.01     |\n",
      "| entropy             | 9.34     |\n",
      "| explained_variance  | 0.237    |\n",
      "| fps                 | 710      |\n",
      "| loss                | 0.611    |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 0.314    |\n",
      "| loss_policy         | 0.314    |\n",
      "| loss_q              | 0.781    |\n",
      "| mean_episode_length | 275      |\n",
      "| mean_episode_reward | 54.2     |\n",
      "| norm_grads          | 19.5     |\n",
      "| norm_grads_policy   | 1.54     |\n",
      "| norm_grads_q        | 19.5     |\n",
      "| total_timesteps     | 70020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0        |\n",
      "| avg_norm_g          | 4.44     |\n",
      "| avg_norm_grads_f    | 4.44     |\n",
      "| avg_norm_k          | 2.16     |\n",
      "| avg_norm_k_dot_g    | 5.06     |\n",
      "| entropy             | 17.1     |\n",
      "| explained_variance  | 0.874    |\n",
      "| fps                 | 713      |\n",
      "| loss                | 2.07     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 1.22     |\n",
      "| loss_policy         | 1.22     |\n",
      "| loss_q              | 2.03     |\n",
      "| mean_episode_length | 268      |\n",
      "| mean_episode_reward | 46       |\n",
      "| norm_grads          | 39.5     |\n",
      "| norm_grads_policy   | 6.64     |\n",
      "| norm_grads_q        | 39       |\n",
      "| total_timesteps     | 72020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 1.3      |\n",
      "| avg_norm_g          | 6.5      |\n",
      "| avg_norm_grads_f    | 5.68     |\n",
      "| avg_norm_k          | 2.1      |\n",
      "| avg_norm_k_dot_g    | 6.73     |\n",
      "| entropy             | 21.8     |\n",
      "| explained_variance  | 0.763    |\n",
      "| fps                 | 715      |\n",
      "| loss                | -0.277   |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -2.07    |\n",
      "| loss_policy         | -2.07    |\n",
      "| loss_q              | 4.02     |\n",
      "| mean_episode_length | 240      |\n",
      "| mean_episode_reward | 34.8     |\n",
      "| norm_grads          | 57.9     |\n",
      "| norm_grads_policy   | 3.73     |\n",
      "| norm_grads_q        | 57.8     |\n",
      "| total_timesteps     | 74020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0        |\n",
      "| avg_norm_g          | 9.45     |\n",
      "| avg_norm_grads_f    | 9.45     |\n",
      "| avg_norm_k          | 2.03     |\n",
      "| avg_norm_k_dot_g    | 9.57     |\n",
      "| entropy             | 17.8     |\n",
      "| explained_variance  | 0.521    |\n",
      "| fps                 | 718      |\n",
      "| loss                | 9.42     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 2.58     |\n",
      "| loss_policy         | 2.58     |\n",
      "| loss_q              | 14       |\n",
      "| mean_episode_length | 230      |\n",
      "| mean_episode_reward | 14.1     |\n",
      "| norm_grads          | 100      |\n",
      "| norm_grads_policy   | 19.6     |\n",
      "| norm_grads_q        | 98.3     |\n",
      "| total_timesteps     | 76020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0.522    |\n",
      "| avg_norm_g          | 3.24     |\n",
      "| avg_norm_grads_f    | 2.84     |\n",
      "| avg_norm_k          | 2.07     |\n",
      "| avg_norm_k_dot_g    | 3.64     |\n",
      "| entropy             | 17.3     |\n",
      "| explained_variance  | 0.981    |\n",
      "| fps                 | 720      |\n",
      "| loss                | -0.772   |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -0.887   |\n",
      "| loss_policy         | -0.887   |\n",
      "| loss_q              | 0.575    |\n",
      "| mean_episode_length | 240      |\n",
      "| mean_episode_reward | 14.6     |\n",
      "| norm_grads          | 25.5     |\n",
      "| norm_grads_policy   | 3.87     |\n",
      "| norm_grads_q        | 25.3     |\n",
      "| total_timesteps     | 78020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 2.35     |\n",
      "| avg_norm_g          | 9.09     |\n",
      "| avg_norm_grads_f    | 7.68     |\n",
      "| avg_norm_k          | 2.03     |\n",
      "| avg_norm_k_dot_g    | 8.94     |\n",
      "| entropy             | 18.3     |\n",
      "| explained_variance  | 0.653    |\n",
      "| fps                 | 720      |\n",
      "| loss                | 75.1     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -1.57    |\n",
      "| loss_policy         | -1.57    |\n",
      "| loss_q              | 154      |\n",
      "| mean_episode_length | 235      |\n",
      "| mean_episode_reward | -0.818   |\n",
      "| norm_grads          | 84.7     |\n",
      "| norm_grads_policy   | 6.03     |\n",
      "| norm_grads_q        | 84.5     |\n",
      "| total_timesteps     | 80020    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| avg_norm_adj        | 1.1      |\n",
      "| avg_norm_g          | 6.63     |\n",
      "| avg_norm_grads_f    | 5.92     |\n",
      "| avg_norm_k          | 2.25     |\n",
      "| avg_norm_k_dot_g    | 7.07     |\n",
      "| entropy             | 21.4     |\n",
      "| explained_variance  | 0.348    |\n",
      "| fps                 | 720      |\n",
      "| loss                | 0.279    |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -1.23    |\n",
      "| loss_policy         | -1.23    |\n",
      "| loss_q              | 3.45     |\n",
      "| mean_episode_length | 252      |\n",
      "| mean_episode_reward | -4.58    |\n",
      "| norm_grads          | 35.5     |\n",
      "| norm_grads_policy   | 3.29     |\n",
      "| norm_grads_q        | 35.3     |\n",
      "| total_timesteps     | 82020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 11.9     |\n",
      "| avg_norm_g          | 53.6     |\n",
      "| avg_norm_grads_f    | 50       |\n",
      "| avg_norm_k          | 1.65     |\n",
      "| avg_norm_k_dot_g    | 29.2     |\n",
      "| entropy             | 18       |\n",
      "| explained_variance  | -8.55    |\n",
      "| fps                 | 722      |\n",
      "| loss                | 1.05     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -2.09    |\n",
      "| loss_policy         | -2.09    |\n",
      "| loss_q              | 6.63     |\n",
      "| mean_episode_length | 267      |\n",
      "| mean_episode_reward | -19.3    |\n",
      "| norm_grads          | 69.3     |\n",
      "| norm_grads_policy   | 26.8     |\n",
      "| norm_grads_q        | 63.9     |\n",
      "| total_timesteps     | 84020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0        |\n",
      "| avg_norm_g          | 30.1     |\n",
      "| avg_norm_grads_f    | 30.1     |\n",
      "| avg_norm_k          | 1.84     |\n",
      "| avg_norm_k_dot_g    | 28       |\n",
      "| entropy             | 9.8      |\n",
      "| explained_variance  | 0.0672   |\n",
      "| fps                 | 722      |\n",
      "| loss                | 98.9     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 7.3      |\n",
      "| loss_policy         | 7.3      |\n",
      "| loss_q              | 183      |\n",
      "| mean_episode_length | 279      |\n",
      "| mean_episode_reward | -21.6    |\n",
      "| norm_grads          | 561      |\n",
      "| norm_grads_policy   | 33       |\n",
      "| norm_grads_q        | 560      |\n",
      "| total_timesteps     | 86020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0        |\n",
      "| avg_norm_g          | 24.9     |\n",
      "| avg_norm_grads_f    | 24.9     |\n",
      "| avg_norm_k          | 1.82     |\n",
      "| avg_norm_k_dot_g    | 21.4     |\n",
      "| entropy             | 17.2     |\n",
      "| explained_variance  | 0.176    |\n",
      "| fps                 | 725      |\n",
      "| loss                | 11.7     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 4.32     |\n",
      "| loss_policy         | 4.32     |\n",
      "| loss_q              | 15.1     |\n",
      "| mean_episode_length | 283      |\n",
      "| mean_episode_reward | -25.3    |\n",
      "| norm_grads          | 115      |\n",
      "| norm_grads_policy   | 8.73     |\n",
      "| norm_grads_q        | 115      |\n",
      "| total_timesteps     | 88020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 5.94     |\n",
      "| avg_norm_g          | 50.4     |\n",
      "| avg_norm_grads_f    | 47       |\n",
      "| avg_norm_k          | 1.86     |\n",
      "| avg_norm_k_dot_g    | 52.2     |\n",
      "| entropy             | 16.9     |\n",
      "| explained_variance  | -0.175   |\n",
      "| fps                 | 728      |\n",
      "| loss                | 273      |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 1.29     |\n",
      "| loss_policy         | 1.29     |\n",
      "| loss_q              | 543      |\n",
      "| mean_episode_length | 271      |\n",
      "| mean_episode_reward | -34.7    |\n",
      "| norm_grads          | 309      |\n",
      "| norm_grads_policy   | 89.5     |\n",
      "| norm_grads_q        | 296      |\n",
      "| total_timesteps     | 90020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 83.5     |\n",
      "| avg_norm_g          | 315      |\n",
      "| avg_norm_grads_f    | 270      |\n",
      "| avg_norm_k          | 1.93     |\n",
      "| avg_norm_k_dot_g    | 310      |\n",
      "| entropy             | 20       |\n",
      "| explained_variance  | 0.263    |\n",
      "| fps                 | 730      |\n",
      "| loss                | 2.84e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -98.8    |\n",
      "| loss_policy         | -98.8    |\n",
      "| loss_q              | 5.87e+03 |\n",
      "| mean_episode_length | 248      |\n",
      "| mean_episode_reward | -29.6    |\n",
      "| norm_grads          | 2.72e+03 |\n",
      "| norm_grads_policy   | 82.4     |\n",
      "| norm_grads_q        | 2.72e+03 |\n",
      "| total_timesteps     | 92020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 1.57     |\n",
      "| avg_norm_g          | 8.66     |\n",
      "| avg_norm_grads_f    | 7.77     |\n",
      "| avg_norm_k          | 1.9      |\n",
      "| avg_norm_k_dot_g    | 8.15     |\n",
      "| entropy             | 14.9     |\n",
      "| explained_variance  | -0.155   |\n",
      "| fps                 | 732      |\n",
      "| loss                | 7.46     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -1.22    |\n",
      "| loss_policy         | -1.22    |\n",
      "| loss_q              | 17.7     |\n",
      "| mean_episode_length | 247      |\n",
      "| mean_episode_reward | -21.6    |\n",
      "| norm_grads          | 66       |\n",
      "| norm_grads_policy   | 2.3      |\n",
      "| norm_grads_q        | 65.9     |\n",
      "| total_timesteps     | 94020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 1.69     |\n",
      "| avg_norm_g          | 7.47     |\n",
      "| avg_norm_grads_f    | 6.45     |\n",
      "| avg_norm_k          | 1.96     |\n",
      "| avg_norm_k_dot_g    | 7.46     |\n",
      "| entropy             | 18.1     |\n",
      "| explained_variance  | 0.476    |\n",
      "| fps                 | 735      |\n",
      "| loss                | -0.0506  |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -2.37    |\n",
      "| loss_policy         | -2.37    |\n",
      "| loss_q              | 5.01     |\n",
      "| mean_episode_length | 235      |\n",
      "| mean_episode_reward | -34.1    |\n",
      "| norm_grads          | 71.2     |\n",
      "| norm_grads_policy   | 0.959    |\n",
      "| norm_grads_q        | 71.2     |\n",
      "| total_timesteps     | 96020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 9.35     |\n",
      "| avg_norm_g          | 43.2     |\n",
      "| avg_norm_grads_f    | 38.1     |\n",
      "| avg_norm_k          | 2        |\n",
      "| avg_norm_k_dot_g    | 43.7     |\n",
      "| entropy             | 12.1     |\n",
      "| explained_variance  | 0.801    |\n",
      "| fps                 | 737      |\n",
      "| loss                | 1.58     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -0.921   |\n",
      "| loss_policy         | -0.921   |\n",
      "| loss_q              | 5.24     |\n",
      "| mean_episode_length | 233      |\n",
      "| mean_episode_reward | -43.9    |\n",
      "| norm_grads          | 32.5     |\n",
      "| norm_grads_policy   | 20.8     |\n",
      "| norm_grads_q        | 25       |\n",
      "| total_timesteps     | 98020    |\n",
      "----------------------------------\n",
      "Reward:11.351892 +/- 80.377914\n"
     ]
    }
   ],
   "source": [
    "# Creacción del entorno\n",
    "env = gym.make(ENVIRONMENT_NAME)\n",
    "\n",
    "# Instancia del Algoritmo de aprendizaje\n",
    "model = ACER(MlpPolicy, env, verbose=1)\n",
    "\n",
    "# Entrenamiento del Agente, indicando el número de acciones que tiene que realizar (en varios episodios)\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "# Obtenemos información del entrenamiento del agente con la función evaluate_policy(), indicando el numero de episodios de evaluación\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "print(\"Reward:{:2f} +/- {:2f}\".format(mean_reward, std_reward))\n",
    "\n",
    "# Exportación del Agente Entrenado\n",
    "model.save(FILE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9211806e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 5.- Importamos el modelo y lo ponemos a jugar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88f0cb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
      "Score Game:15.385338285620875\n",
      "Steps Game:193\n"
     ]
    }
   ],
   "source": [
    "# Importamos el modelo ya entrenado\n",
    "trained_model = ACER.load(FILE_MODEL_NAME)\n",
    "\n",
    "# Instanciamos el entorno del juego Lunar Lander\n",
    "env = gym.make(ENVIRONMENT_NAME)\n",
    "\n",
    "# Vaciamos la memoria de las observaciones (Lista de estados)\n",
    "obs = env.reset()\n",
    "\n",
    "# ! A JUGAR ¡ (mientras no se termine la partida)\n",
    "done = False\n",
    "score = 0 \n",
    "steps = 0\n",
    "while not done:\n",
    "    action, _states = trained_model.predict(obs) # Dada una observación, pedimos al agente la acción a realizar\n",
    "    obs, rewards, done, info = env.step(action)  # Realizamos la acción y nos devuelve:\n",
    "                                                 # 1. Lista de estados, 2. Recompensa, 3. ¿Fin del juego?, 4. info\n",
    "    score += rewards  # Sumamos la recompensa\n",
    "    steps +=1         # Sumamos los pasos\n",
    "    env.render()\n",
    "    \n",
    "print('Score Game:{}'.format(score))\n",
    "print('Steps Game:{}'.format(steps))\n",
    "\n",
    "# Cerramos el entorno\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
